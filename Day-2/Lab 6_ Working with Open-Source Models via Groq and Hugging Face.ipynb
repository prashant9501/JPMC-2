{"cells":[{"cell_type":"markdown","source":["-----\n","\n","### **Course: Advanced Prompt Engineering for Banking**\n","\n","### **Lab 6: Working with Open-Source Models via Groq and Hugging Face**\n","\n","**Objective:** This lab will guide you through setting up and using powerful open-source models from providers like Groq (for speed) and Hugging Face (for variety). You will learn how to manage API keys and access tokens, create reusable functions to interact with these services, and apply the same prompting techniques to different underlying models.\n","\n","-----\n","\n","### **1. Setup and Connection Instructions**\n","\n","First, we need to install the specific client libraries for Groq and Hugging Face.\n","\n","#### **1.1 Installing Libraries**"],"metadata":{"id":"BgtrBLZW-Dzx"}},{"cell_type":"code","source":["# Run this cell to install the necessary packages\n","!pip install groq==0.13.0\n","!pip install huggingface_hub==0.26.2"],"outputs":[],"execution_count":null,"metadata":{"id":"1MXp3VNh-Dzz"}},{"cell_type":"markdown","source":["#### **1.2 Connecting to Groq Cloud** ‚òÅÔ∏è\n","\n","To use Groq's Llama 3.2 model, you need a free API key.\n","\n","**How to get your Groq API Key:**\n","\n","1.  Go to the Groq Cloud console: `https://console.groq.com/`\n","2.  Sign up or log in to your account.\n","3.  In the left-hand menu, navigate to **API Keys**.\n","4.  Click the \"Create API Key\" button. Give it a name (e.g., \"colab-notebook-key\") and click \"Create\".\n","5.  **Important:** Copy the key immediately and save it somewhere secure. You will not be able to see it again after you close the window.\n","\n","Now, run the following Python code to securely input your key."],"metadata":{"id":"oAF1cDwD-Dzz"}},{"cell_type":"code","source":["import os\n","# from getpass import getpass\n","\n","# Securely get the Groq API key\n","# groq_api_key = getpass(\"Enter your Groq API Key: \")\n","# os.environ[\"GROQ_API_KEY\"] = groq_api_key"],"outputs":[],"execution_count":null,"metadata":{"id":"5BkM5hEQ-Dz0"}},{"cell_type":"markdown","source":["#### **1.3 Connecting to Hugging Face ü§ó**\n","\n","To use models from Hugging Face, you need an Access Token. Many powerful models, including Meta's Llama models, are \"gated\" and require you to accept their terms before you can access them.\n","\n","**How to get your Hugging Face Access Token:**\n","\n","1.  Go to the Hugging Face website: `https://huggingface.co/`\n","2.  Sign up or log in to your account.\n","3.  Click on your profile picture in the top-right corner and go to **Settings**.\n","4.  In the left-hand menu, navigate to **Access Tokens**.\n","5.  Click the \"New token\" button. Give it a name and assign it a \"write\" role, then click \"Generate a token\".\n","6.  Copy the token and save it.\n","\n","**How to Access Gated Models (like Llama 3.2):**\n","Before you can use a gated model via API, you must accept its terms on the website.\n","\n","1.  Navigate to the model's page, for example: `https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct`\n","2.  You will see a prompt asking you to agree to the terms and share your information.\n","3.  Review the license and submit the form. Approval is often instant but can sometimes take a short while. **You must complete this step to avoid \"access forbidden\" errors.**\n","\n","Now, run the following code to securely input your token."],"metadata":{"id":"OHCYLtfK-Dz0"}},{"cell_type":"code","source":["# from getpass import getpass\n","\n","# Securely get the Hugging Face Access Token\n","# hf_token = getpass(\"Enter your Hugging Face Access Token: \")\n","# os.environ[\"HF_TOKEN\"] = hf_token"],"outputs":[],"execution_count":null,"metadata":{"id":"tKm4hrX3-Dz0"}},{"cell_type":"markdown","source":["-----\n","\n","### **2. Creating Helper Functions**\n","\n","Now, we'll create the two new execution functions as you specified. One for Groq and one for Hugging Face."],"metadata":{"id":"SVBzvEfI-Dz0"}},{"cell_type":"code","source":["from groq import Groq\n","from huggingface_hub import InferenceClient\n","from langchain_core.prompts import PromptTemplate\n","from IPython.display import Markdown, display\n","\n","# Initialize clients\n","# groq_client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n","groq_client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n","\n","# The HF client is initialized within the function as the model can change\n","\n","def execute_prompt_groq(prompt_template, title, model_name=\"llama-3.2-90b-vision-preview\", **kwargs):\n","    \"\"\"\n","    Executes a prompt using the Groq API and prints the output.\n","    Uses a default model if none is provided.\n","    \"\"\"\n","    print(f\"--- {title} (Provider: Groq, Model: {model_name}) ---\")\n","\n","    prompt = PromptTemplate.from_template(prompt_template)\n","    final_prompt_text = prompt.format(**kwargs)\n","    print(\"PROMPT:\")\n","    print(final_prompt_text)\n","\n","    chat_completion = groq_client.chat.completions.create(\n","        messages=[\n","            {\n","                \"role\": \"user\",\n","                \"content\": final_prompt_text,\n","            }\n","        ],\n","        model=model_name,\n","        temperature=0.0,\n","    )\n","\n","    response = chat_completion.choices[0].message.content\n","    print(\"\\n\" + \"=\"*20 + \" RESPONSE \" + \"=\"*20 + \"\\n\")\n","    display(Markdown(response))\n","    print(\"\\n\" + \"-\"*50 + \"\\n\")\n","\n","\n","def execute_prompt_hf(prompt_template, title, model_name, **kwargs):\n","    \"\"\"\n","    Executes a prompt using the Hugging Face Inference API and prints the output.\n","    Requires the model name to be passed explicitly.\n","    \"\"\"\n","    print(f\"--- {title} (Provider: Hugging Face, Model: {model_name}) ---\")\n","\n","    # Initialize client with the specified model\n","    hf_client = InferenceClient(model=model_name, token=os.environ.get(\"HF_TOKEN\"))\n","\n","    prompt = PromptTemplate.from_template(prompt_template)\n","    final_prompt_text = prompt.format(**kwargs)\n","    print(\"PROMPT:\")\n","    print(final_prompt_text)\n","\n","    # The HF Inference Client uses a different format for messages\n","    response = hf_client.chat_completion(\n","        messages=[{\"role\": \"user\", \"content\": final_prompt_text}],\n","        max_tokens=500,\n","        temperature=0.1\n","    )\n","\n","    print(\"\\n\" + \"=\"*20 + \" RESPONSE \" + \"=\"*20 + \"\\n\")\n","    display(Markdown(response.choices[0].message.content))\n","    print(\"\\n\" + \"-\"*50 + \"\\n\")"],"outputs":[],"execution_count":null,"metadata":{"id":"JVPS_jJq-Dz1"}},{"cell_type":"markdown","source":["-----\n","\n","### **3. Applying Prompting Techniques: Zero-Shot Classification**\n","\n","Here is the implementation for the first use case. We will classify the sentiment of a financial news headline using both Groq and Hugging Face back-to-back.\n","\n","**Scenario:** An automated system for a large investment bank needs to classify the sentiment of news headlines. The output must be a single, standardized word (`Positive`, `Negative`, `Neutral`)."],"metadata":{"id":"U0epfG6_-Dz1"}},{"cell_type":"code","source":["news_headline = \"Supply chain disruptions in Asia are expected to cause a Q4 earnings dip for major tech stocks, warns analyst.\""],"outputs":[],"execution_count":null,"metadata":{"id":"8tN1u0f8-Dz1"}},{"cell_type":"markdown","source":["#### **Groq Implementation (Llama 3.2 90B)**\n","\n","**Naive Prompt üëé**"],"metadata":{"id":"Nl9YgrSp-Dz1"}},{"cell_type":"code","source":["naive_prompt_1 = \"What do you think the sentiment of this news headline is?\\n\\nHeadline: \\\"{headline}\\\"\"\n","\n","execute_prompt_groq(\n","    naive_prompt_1,\n","    \"1.1 Naive Sentiment Classification\",\n","    headline=news_headline\n",")"],"outputs":[],"execution_count":null,"metadata":{"id":"4Whe06Vz-Dz2"}},{"cell_type":"markdown","source":["**Improved Prompt üëç**"],"metadata":{"id":"BiSVhnIN-Dz2"}},{"cell_type":"code","source":["improved_prompt_1 = \"\"\"\n","Act as a financial market analyst AI.\n","Given the following news headline, classify the market sentiment.\n","\n","The sentiment must be one of the following: Positive, Negative, or Neutral.\n","Return only the single-word sentiment category and nothing else.\n","\n","Headline: \"{headline}\"\n","\"\"\"\n","execute_prompt_groq(\n","    improved_prompt_1,\n","    \"1.2 Improved Zero-Shot Classification\",\n","    headline=news_headline\n",")"],"outputs":[],"execution_count":null,"metadata":{"id":"1P3f8evV-Dz2"}},{"cell_type":"markdown","source":["#### **Hugging Face Implementation (Llama 3.2 1B)**\n","\n","**Naive Prompt üëé**"],"metadata":{"id":"dCYk971h-Dz2"}},{"cell_type":"code","source":["# We use the same naive prompt text, but call the Hugging Face function\n","execute_prompt_hf(\n","    naive_prompt_1,\n","    \"1.3 Naive Sentiment Classification\",\n","    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n","    headline=news_headline\n",")"],"outputs":[],"execution_count":null,"metadata":{"id":"v1nRX6AR-Dz2"}},{"cell_type":"markdown","source":["**Improved Prompt üëç**"],"metadata":{"id":"9Wk0Qqae-Dz2"}},{"cell_type":"code","source":["# We use the same improved prompt text, but call the Hugging Face function\n","execute_prompt_hf(\n","    improved_prompt_1,\n","    \"1.4 Improved Zero-Shot Classification\",\n","    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n","    headline=news_headline\n",")"],"outputs":[],"execution_count":null,"metadata":{"id":"XNHF8g3K-Dz2"}},{"cell_type":"markdown","source":["You can now use these two functions, `execute_prompt_groq` and `execute_prompt_hf`, to complete the remaining 12 use cases from our previous labs. Just remember to pass the `model_name` for the Hugging Face function calls."],"metadata":{"id":"QX0MTZKc-Dz2"}},{"cell_type":"code","source":[],"metadata":{"id":"MQPUmVmy_M88"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}