{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Course: Advanced Prompt Engineering for Banking**\n",
        "\n",
        "### **Lab 4: Dynamic and Verifiable Prompting Frameworks**\n",
        "\n",
        "**Objective:** This lab explores advanced frameworks that enable LLMs to interact with external systems and verify their own statements. You will learn to implement **ReAct (Reason and Act)** to build tool-using agents and **Chain-of-Verification (CoVe)** to enhance factual accuracy, both critical for creating reliable AI solutions in finance.\n",
        "\n",
        "-----\n",
        "### Created By: Prashant Sahu\n",
        "[Connect with me on LinkedIn](https://www.linkedin.com/in/prashantksahu/)\n",
        "\n",
        "-----\n",
        "-----\n",
        "</br>\n",
        "\n",
        "\n",
        "\n",
        "### **Setup**\n",
        "\n",
        "This lab requires the `langchain-community` and `langchainhub` libraries for agent creation.\n",
        "\n",
        "```bash\n",
        "pip install langchain langchain-openai python-dotenv langchain-community langchainhub\n",
        "```\n",
        "\n",
        "**lab\\_script.py:**"
      ],
      "metadata": {
        "id": "nqOTZf9r2vnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain.tools import tool\n",
        "from langchain import hub\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
        "\n",
        "# Helper function for simple prompt execution (for CoVe)\n",
        "def execute_prompt(prompt_template, title, **kwargs):\n",
        "    print(f\"--- {title} ---\")\n",
        "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    response = chain.invoke(kwargs)\n",
        "    print(\"PROMPT:\")\n",
        "    print(prompt.format(**kwargs))\n",
        "    print(\"\\n\" + \"=\"*20 + \" RESPONSE \" + \"=\"*20 + \"\\n\")\n",
        "    print(response)\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "    return response"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "5bX6b_4x2vnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## **1. ReAct (Reason and Act): Giving LLMs Tools**\n",
        "\n",
        "### **Background**\n",
        "\n",
        "**ReAct** is a paradigm that transforms a language model from a simple text generator into a reasoning agent that can interact with its environment. It synergizes reasoning (like in Chain-of-Thought) with the ability to take actions.\n",
        "\n",
        "  * **How it works:** The LLM operates in a loop: **Thought** -\\> **Action** -\\> **Observation**.\n",
        "    1.  **Thought:** The model thinks about the user's query and decides what it needs to do.\n",
        "    2.  **Action:** It chooses a `tool` (e.g., a database query, an API call) and the `input` for that tool.\n",
        "    3.  **Observation:** The tool executes, and its output is fed back to the model as an observation.\n",
        "        This loop continues until the model has enough information to form a final answer.\n",
        "  * **Strengths:** Allows LLMs to overcome their inherent limitations, such as knowledge cutoffs and the inability to access private data. It makes them dynamic problem-solvers that can fetch real-time information.\n",
        "  * **Banking Use Cases:** Checking live stock prices, fetching a customer's account balance from a secure database, calculating loan EMIs using a precise financial calculator, or looking up current forex rates before executing a trade.\n",
        "\n",
        "### **Use Case 1.1: Customer Account & Transaction Inquiry**\n",
        "\n",
        "**Scenario:** A customer service chatbot needs to answer a specific question about a customer's account balance and recent transactions.\n",
        "\n",
        "#### **Naive Prompt (Non-ReAct) ðŸ‘Ž**\n",
        "\n",
        "The model has no access to real-time, private data. It will either refuse to answer or, worse, **hallucinate** an answer."
      ],
      "metadata": {
        "id": "lds4N5tv2vnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a simple execution without any tools\n",
        "naive_prompt_react_1 = \"Hi, can you tell me the current balance for account number 1002345 and list my last 2 transactions?\"\n",
        "\n",
        "# execute_prompt(\n",
        "#     \"{query}\",\n",
        "#     \"1.1 Naive Account Inquiry\",\n",
        "#     query=naive_prompt_react_1\n",
        "# )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "xapmg42u2vnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected Response:**\n",
        "\n",
        "```\n",
        "I am an AI model and do not have access to real-time, private banking information. To get your account balance and transaction history, please log in to your official banking app or contact customer support.\n",
        "```\n",
        "\n",
        "This is a safe but unhelpful response.\n",
        "\n",
        "#### **ReAct Implementation ðŸ‘**\n",
        "\n",
        "Here, we define mock \"tools\" that the LLM can use. In a real application, these would be secure API calls to the bank's core systems."
      ],
      "metadata": {
        "id": "fVd6yzc72vnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Tool Definition ---\n",
        "# We define simple Python functions as our tools.\n",
        "@tool\n",
        "def get_account_balance(account_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Returns the current balance for a given bank account ID.\n",
        "    Use this tool to find a customer's account balance.\n",
        "    \"\"\"\n",
        "    print(f\"--- TOOL EXECUTED: get_account_balance({account_id}) ---\")\n",
        "    # Mock data for demonstration\n",
        "    balances = {\"1002345\": \"â‚¹1,50,234.50\", \"9876543\": \"â‚¹8,750.00\"}\n",
        "    return balances.get(account_id, \"Account not found.\")\n",
        "\n",
        "@tool\n",
        "def get_last_n_transactions(account_id: str, n: int) -> str:\n",
        "    \"\"\"\n",
        "    Returns the last 'n' transactions for a given bank account ID.\n",
        "    Use this tool to find a customer's recent transaction history.\n",
        "    \"\"\"\n",
        "    print(f\"--- TOOL EXECUTED: get_last_n_transactions({account_id}, {n}) ---\")\n",
        "    transactions = {\n",
        "        \"1002345\": [\n",
        "            \"2025-09-15 | DEBIT | â‚¹2,500.00 | Merchant: SWIGGY\",\n",
        "            \"2025-09-14 | DEBIT | â‚¹12,000.00 | Merchant: AMAZON\",\n",
        "            \"2025-09-12 | CREDIT | â‚¹75,000.00 | Source: SALARY CREDIT\"\n",
        "        ]\n",
        "    }\n",
        "    return str(transactions.get(account_id, \"Account not found.\")[:n])\n",
        "\n",
        "# --- Agent Creation ---\n",
        "# Pull the ReAct prompt template\n",
        "prompt = hub.pull(\"hwchase17/react\")\n",
        "\n",
        "# List the tools the agent can use\n",
        "tools = [get_account_balance, get_last_n_transactions]\n",
        "\n",
        "# Create the agent\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# --- Agent Execution ---\n",
        "# query = \"Hi, can you tell me the current balance for account number 1002345 and list my last 2 transactions?\"\n",
        "# response = agent_executor.invoke({\"input\": query})\n",
        "# print(\"--- FINAL AGENT RESPONSE ---\")\n",
        "# print(response['output'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "qNMYWKW_2vnG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mtU7-PeyvGUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ReAct Technique vs. LangChain's ReAct Agent**\n",
        "\n",
        "#### Question: Is ReAct Technique same as creating a ReAct Agent?\n",
        "No, the **ReAct prompting technique** and creating a **ReAct agent in LangChain** are not exactly the same, but they are directly related.\n",
        "\n",
        "  * **ReAct Technique:** This is the **conceptual framework** first described in a research paper. It establishes the idea that a language model can achieve complex tasks by interleaving `Reasoning` (thinking about what to do) and `Acting` (using a tool to get external information). It's a blueprint for how an agent should think.\n",
        "\n",
        "  * **LangChain ReAct Agent:** This is a specific, practical **implementation** of the ReAct technique. LangChain provides the scaffolding (code, prompts, and parsers) to make an LLM follow the \"Thought -\\> Action -\\> Observation\" loop easily. It's the ready-to-use engine built according to the ReAct blueprint.\n",
        "\n",
        "#### Question: Can we implement the ReAct Technique without creating a ReAct Agent?\n",
        "**No, you cannot meaningfully implement the ReAct framework without tools.** The **\"Act\"** part of ReAct is its entire purpose. It's what separates it from a simple Chain-of-Thought prompt. The action is always the use of a tool, whether that tool is a web search, a database query, a calculator, or a call to another API. Without a tool to \"Act\" upon, the framework collapses into just a \"Reason\" step, which is essentially Chain-of-Thought.\n",
        "\n",
        "-----\n"
      ],
      "metadata": {
        "id": "9aaN_UBSvG5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ReAct Use Case 1.2: Real-Time Forex Rate and Trade Feasibility**\n",
        "\n",
        "**Scenario:** A corporate treasurer needs to know the live USD/INR exchange rate and confirm if the bank has enough liquidity to handle a large trade of $500,000 immediately.\n",
        "\n",
        "#### **Naive Prompt (Non-ReAct) ðŸ‘Ž**\n",
        "\n",
        "The LLM lacks real-time market data and internal bank information, forcing it to be vague or provide stale, incorrect data."
      ],
      "metadata": {
        "id": "rsVHczr1vrpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_prompt_react_2 = \"What's the current USD/INR exchange rate, and can I execute a $500,000 trade right now?\"\n",
        "\n",
        "# execute_prompt(\n",
        "#     \"{query}\",\n",
        "#     \"1.2 Naive FX Inquiry\",\n",
        "#     query=naive_prompt_react_2\n",
        "# )"
      ],
      "metadata": {
        "id": "t7kxp38Lvv3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ReAct Implementation ðŸ‘**\n",
        "\n",
        "We provide the agent with tools to fetch live data and check internal systems, allowing it to give a complete, actionable answer."
      ],
      "metadata": {
        "id": "DfWER5INvrnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- New Tool Definitions ---\n",
        "@tool\n",
        "def get_forex_rate(base_currency: str, quote_currency: str) -> float:\n",
        "    \"\"\"\n",
        "    Returns the real-time exchange rate for a given currency pair.\n",
        "    Use this to get current foreign exchange rates.\n",
        "    \"\"\"\n",
        "    print(f\"--- TOOL EXECUTED: get_forex_rate({base_currency}, {quote_currency}) ---\")\n",
        "    # Mock live data for demonstration\n",
        "    if base_currency == \"USD\" and quote_currency == \"INR\":\n",
        "        return 83.54\n",
        "    return 0.0\n",
        "\n",
        "@tool\n",
        "def check_trade_feasibility(currency: str, amount: float) -> str:\n",
        "    \"\"\"\n",
        "    Checks if a trade of a certain size is feasible based on the bank's current liquidity and client limits.\n",
        "    Use this to verify if a large trade can be executed.\n",
        "    \"\"\"\n",
        "    print(f\"--- TOOL EXECUTED: check_trade_feasibility({currency}, {amount}) ---\")\n",
        "    if currency == \"USD\" and amount <= 1000000:\n",
        "        return \"Feasible: The trade is within the current liquidity limits for this client.\"\n",
        "    else:\n",
        "        return \"Not Feasible: The trade amount exceeds the bank's current single-trade limit. Please contact the trading desk.\"\n",
        "\n",
        "# --- Agent Creation & Execution ---\n",
        "# We can reuse the same agent setup from the previous example, just with new tools.\n",
        "prompt = hub.pull(\"hwchase17/react\")\n",
        "tools_fx = [get_forex_rate, check_trade_feasibility]\n",
        "agent_fx = create_react_agent(llm, tools_fx, prompt)\n",
        "agent_executor_fx = AgentExecutor(agent=agent_fx, tools=tools_fx, verbose=True)\n",
        "\n",
        "# query = \"What's the current USD/INR exchange rate, and can I execute a $500,000 trade right now?\"\n",
        "# response = agent_executor_fx.invoke({\"input\": query})\n",
        "# print(\"--- FINAL AGENT RESPONSE ---\")\n",
        "# print(response['output'])"
      ],
      "metadata": {
        "id": "ZY_00BnQv7k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Rl5QtGjAvrkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## **2. Chain-of-Verification (CoVe): Minimizing Hallucinations**\n",
        "\n",
        "### **Background**\n",
        "\n",
        "Factual accuracy is non-negotiable in banking. **Chain-of-Verification (CoVe)** is a technique designed to make an LLM double-check its own work, significantly reducing the risk of generating plausible-sounding but incorrect information (hallucinations).\n",
        "\n",
        "  * **How it works:** It's a deliberate, multi-step process.\n",
        "    1.  **Draft Response:** The LLM generates an initial answer to the query.\n",
        "    2.  **Plan Verifications:** The model reviews its own draft and generates a list of factual claims that need to be checked.\n",
        "    3.  **Execute Verifications:** The model answers these verification questions *independently*, which prevents the initial draft from biasing the check.\n",
        "    4.  **Final Verified Response:** The model revises the initial draft based on the outcome of the verifications, correcting any inaccuracies.\n",
        "  * **Strengths:** Drastically improves the factual grounding and reliability of generated text. It's an essential technique for high-stakes content generation.\n",
        "  * **Banking Use Cases:** Summarizing complex regulatory documents (like RBI circulars), generating financial reports, creating internal policy documentation, or drafting market analysis where every claim must be backed by data.\n",
        "\n",
        "### **Use Case 2.1: Summarizing a Regulatory Guideline**\n",
        "\n",
        "**Scenario:** A compliance officer needs a quick, accurate summary of a new (fictional) RBI guideline on digital lending."
      ],
      "metadata": {
        "id": "Y7QLSe2s2vnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rbi_guideline_text = \"\"\"\n",
        "RBI Circular No. 2025/11/A-34\n",
        "Subject: Guidelines on Digital Lending for Non-Banking Financial Companies (NBFCs)\n",
        "\n",
        "1. Introduction: To ensure customer protection and data security, all NBFCs engaged in digital lending must adhere to the following.\n",
        "2. Loan Disbursal: All loan disbursals must be executed directly from the NBFC's bank account to the borrower's bank account. No third-party pool accounts are permitted. This rule is effective from December 1, 2025.\n",
        "3. Data Privacy: Only minimal, necessary customer data may be collected. Explicit customer consent is required for any data sharing. Biometric data cannot be stored.\n",
        "4. Applicability: These guidelines apply to all NBFCs. Regional Rural Banks (RRBs) are currently exempt.\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "1G3Z-8iC2vnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Naive Prompt ðŸ‘Ž**\n",
        "\n",
        "A simple summarization request can lead to subtle but critical errors of omission or misinterpretation."
      ],
      "metadata": {
        "id": "MT7YvGrT2vnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# naive_prompt_cove_1 = \"Summarize the key points of the following RBI guideline: \\n\\n{document}\"\n",
        "# naive_summary = execute_prompt(\n",
        "#     naive_prompt_cove_1,\n",
        "#     \"2.1 Naive Guideline Summary\",\n",
        "#     document=rbi_guideline_text\n",
        "# )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Ldkq0bIu2vnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Critique:** The summary is mostly correct, but it makes one critical mistake: it claims the rules apply to \"all financial institutions,\" when the text clearly exempts Regional Rural Banks (RRBs).\n",
        "\n",
        "#### **CoVe Implementation ðŸ‘**\n",
        "\n",
        "We simulate the CoVe process using a sequence of prompts to force self-correction."
      ],
      "metadata": {
        "id": "GnKXVcDc2vnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CoVe Step 1: Draft Initial Response ---\n",
        "step1_prompt = \"Draft a summary of the key points of the following RBI guideline: \\n\\n{document}\"\n",
        "draft_summary = execute_prompt(\n",
        "    step1_prompt,\n",
        "    \"CoVe Step 1: Draft Summary\",\n",
        "    document=rbi_guideline_text\n",
        ")\n",
        "\n",
        "# --- CoVe Step 2: Plan Verifications ---\n",
        "step2_prompt = \"\"\"\n",
        "Based on the following summary, generate a list of verification questions to fact-check its claims against the original document.\n",
        "\n",
        "Summary:\n",
        "{summary}\n",
        "\"\"\"\n",
        "verification_plan = execute_prompt(\n",
        "    step2_prompt,\n",
        "    \"CoVe Step 2: Plan Verifications\",\n",
        "    summary=draft_summary\n",
        ")\n",
        "\n",
        "# --- CoVe Step 3: Execute Verifications (Simulated) ---\n",
        "# In a real system, you'd answer these questions independently. Here, we bundle them.\n",
        "step3_prompt = \"\"\"\n",
        "Answer the following questions based *only* on the provided RBI document.\n",
        "\n",
        "Document:\n",
        "{document}\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\"\"\"\n",
        "verification_results = execute_prompt(\n",
        "    step3_prompt,\n",
        "    \"CoVe Step 3: Execute Verifications\",\n",
        "    document=rbi_guideline_text,\n",
        "    questions=verification_plan\n",
        ")\n",
        "\n",
        "# --- CoVe Step 4: Generate Final Verified Response ---\n",
        "step4_prompt = \"\"\"\n",
        "You have an initial draft summary and a set of verified facts.\n",
        "Your task is to rewrite the summary, correcting any inaccuracies or omissions based on the verified facts.\n",
        "\n",
        "Initial Summary:\n",
        "{summary}\n",
        "\n",
        "Verified Facts (Q&A):\n",
        "{verifications}\n",
        "\n",
        "Final Verified Summary:\n",
        "\"\"\"\n",
        "final_summary = execute_prompt(\n",
        "    step4_prompt,\n",
        "    \"CoVe Step 4: Final Verified Summary\",\n",
        "    summary=draft_summary,\n",
        "    verifications=verification_results\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "KmE5ImdZ2vnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This final version correctly captures the nuance of the exemption, which the naive prompt missed."
      ],
      "metadata": {
        "id": "G7N2s_qw2vnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chain-of-Verification (CoVe) Use Case 2.2: Generating Factual Stock Analysis**\n",
        "\n",
        "**Scenario:** A junior equity analyst needs to write a short, factually precise summary of a company's quarterly performance for an internal client newsletter. The summary must not contain any speculation."
      ],
      "metadata": {
        "id": "E4j-LWaZwDz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fictional quarterly data points for \"Innovate Corp.\"\n",
        "stock_data = \"\"\"\n",
        "- Company: Innovate Corp.\n",
        "- Revenue: â‚¹150 Crores (an increase of 12% Year-over-Year).\n",
        "- Net Profit: â‚¹25 Crores (an increase of 8% Year-over-Year).\n",
        "- Key Highlight: Successfully launched new AI platform 'Cognito'.\n",
        "- Stock Price Movement (last quarter): -5%.\n",
        "- Analyst Consensus Rating: 'Hold'.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gzYOJZFowHo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Naive Prompt ðŸ‘Ž**\n",
        "\n",
        "The model might try to be \"helpful\" by inventing a narrative or a cause-and-effect relationship that isn't supported by the facts provided."
      ],
      "metadata": {
        "id": "MeVoO9t3wJ6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# naive_prompt_cove_2 = \"Write a short paragraph analyzing Innovate Corp's latest quarterly results based on these data points: \\n\\n{data}\"\n",
        "# naive_analysis = execute_prompt(\n",
        "#     naive_prompt_cove_2,\n",
        "#     \"2.2 Naive Stock Analysis\",\n",
        "#     data=stock_data\n",
        "# )"
      ],
      "metadata": {
        "id": "ih2ldA9nwMcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Critique:** The phrase \"market reacted skeptically\" directly and incorrectly links the stock drop to the results, a causal relationship not present in the source data. This is speculation, not factual reporting.\n",
        "\n",
        "#### **CoVe Implementation ðŸ‘**\n",
        "\n",
        "The CoVe process forces the model to check its own claims against the source data, stripping out any speculative connections."
      ],
      "metadata": {
        "id": "ScQjBrDmwPnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CoVe Step 1: Draft Initial Response ---\n",
        "step1_prompt = \"Draft a short paragraph analyzing Innovate Corp's latest quarterly results based on these data points: \\n\\n{data}\"\n",
        "draft_analysis = execute_prompt(\n",
        "    step1_prompt,\n",
        "    \"CoVe Step 1: Draft Analysis\",\n",
        "    data=stock_data\n",
        ")\n",
        "\n",
        "# --- CoVe Step 2: Plan Verifications ---\n",
        "step2_prompt = \"\"\"\n",
        "Based on the following draft analysis, generate a list of verification questions to fact-check its claims against the original data points.\n",
        "\n",
        "Draft Analysis:\n",
        "{analysis}\n",
        "\"\"\"\n",
        "verification_plan_2 = execute_prompt(\n",
        "    step2_prompt,\n",
        "    \"CoVe Step 2: Plan Verifications\",\n",
        "    analysis=draft_analysis\n",
        ")\n",
        "\n",
        "# --- CoVe Step 3: Execute Verifications ---\n",
        "step3_prompt = \"\"\"\n",
        "Answer the following questions based *only* on the provided data points. Do not infer or speculate.\n",
        "\n",
        "Data Points:\n",
        "{data}\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\"\"\"\n",
        "verification_results_2 = execute_prompt(\n",
        "    step3_prompt,\n",
        "    \"CoVe Step 3: Execute Verifications\",\n",
        "    data=stock_data,\n",
        "    questions=verification_plan_2\n",
        ")\n",
        "\n",
        "# --- CoVe Step 4: Generate Final Verified Response ---\n",
        "step4_prompt = \"\"\"\n",
        "You have a draft analysis and a set of verified facts. Your task is to rewrite the analysis, ensuring every statement is directly supported by the verified facts. Remove any speculation or causal links that are not explicitly mentioned in the data.\n",
        "\n",
        "Draft Analysis:\n",
        "{analysis}\n",
        "\n",
        "Verified Facts (Q&A):\n",
        "{verifications}\n",
        "\n",
        "Final Verified Analysis:\n",
        "\"\"\"\n",
        "final_analysis = execute_prompt(\n",
        "    step4_prompt,\n",
        "    \"CoVe Step 4: Final Verified Analysis\",\n",
        "    analysis=draft_analysis,\n",
        "    verifications=verification_results_2\n",
        ")"
      ],
      "metadata": {
        "id": "wEM8Bu_YwTeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This final version is factually impeccable. It presents the different data points without inventing a narrative to connect them, which is exactly what's required for unbiased financial reporting."
      ],
      "metadata": {
        "id": "b2qbOYCSwY1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "i-7y89D1wYwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VMfuN-p_wYrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LyjU8qAlwDtK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}